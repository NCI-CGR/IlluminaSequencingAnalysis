## The delivered data path from USU
1: Directory/data/COVID_WGS/COVNET_Data_Delivery/NCI_COVNET_WGS

2: PLEASE pay attention:
- The data not always in the same folder structure    
  - Some of them may be directly in the root folder (e.g. low input)   
  - Some of them may come with the subfolder (e.g. standard input)

## COVID_Primary pipeline 

### How to run it 
**Step 1:** Create a seperate folder and put all of target delivered folders inside (one time)
- Why?  
  - some new data may come in when you run the command line  
  - the file may not in sub-folder
- How?   
  - Create the soft-link(s) for either sub-folder or exact fastq files.   
  - Manually
- Example:   
  - /data/COVID_WGS/primary_analysis/Data/StdInput/07_02_2021    
  - /data/COVID_WGS/primary_analysis/Data/LowInput/07_26_2021
 
**Step 2:** Reconstruct data (one time)
- Why 
  - The whole pipeline needs to follow the restricted folder structure as our CGR flowcell.
- How 
  - We need to group the samples that belong to the same flowcell together.  
  - We need to use the same folder structure as our CGR flowcell to save all these samples.  
  - We can use the script "DataReconstruct.py".
- Example
```
rawDir="/data/COVID_WGS/primary_analysis/Data/07_02_2021"processedDir="/data/COVID_WGS/primary_analysis/COVID19/08_03_2021/ProcessedData"python3 /home/lix33/lxwg/Git/IlluminaSequencingAnalysis/CustomizedQC/SourceCode/DataReconstruct.py ${rawDir} ${processedDir}
```

**Step 3:** COVID primary analysis (crontab job)
- Run Customized QC python source code
- Input  
  - the processed folder generated by step 2.
- Example
```
1: Command linepython3 /home/lix33/lxwg/Git/IlluminaSequencingAnalysis/CustomizedQC/SourceCode/CustomizedQC.py /data/COVID_WGS/primary_analysis/COVID19/06_30_2021/ProcessedData
2: Crontab job@hourly . /etc/bashrc && source /home/lix33/lxwg/Git/IlluminaSequencingAnalysis/CustomizedQC/Crontab/run.sh >> /home/lix33/lxwg/Project/CustomizedQC/Log/$(date +\%Y\%m).txt 2>&1
```

**Step 4:** Move data from biowulf to S3 (object storage system)
- Why?
  - The storage in biowulf is limited (80T).   
  - We need to move the finished data to S3 (290 TB) to save space in biowulf     
- How to check the quota in s3
  - obj_df -h
- Example
```
python3 /home/lix33/lxwg/Git/IlluminaSequencingAnalysis/CustomizedQC/SourceCode/ObjectStorage/Backup2S3.py
Also check: /home/lix33/lxwg/Test/slurm/object_storage/job.sh
```

**Step 5:** merge qc report together
- Why?  
  - Since one build may be finished by different sub-builds (due to the storage limitation in biowulf), we need to merge the running result once all sub-builds have been finished. 
- Example 
```
python3 /home/lix33/lxwg/Git/IlluminaSequencingAnalysis/CustomizedQC/SourceCode/MergeQCReport.py /data/COVID_WGS/primary_analysis/COVID19/06_30_2021/QCReport
```
